# 22F-3715-Ass-no-02-DS

# Research Paper Scraper & Annotation using LLMs

## ğŸ“Œ Project Overview
This project automates the process of scraping research papers from [NeurIPS](https://papers.nips.cc), storing them locally, and annotating them using a Large Language Model (LLM) API such as Google Gemini. The final dataset includes metadata and an assigned category for each paper, making it easier for researchers to analyze papers offline.

## ğŸš€ Features
- **Scrapes research papers** from NeurIPS for the last five years.
- **Extracts titles and abstracts** from PDFs.
- **Uses an LLM API** (Google Gemini or OpenAI ChatGPT) to classify papers into five categories.
- **Stores metadata and annotations** in structured CSV files.
- **Supports offline access** to the collected research papers and annotations.

## ğŸ“‚ Folder Structure
```
ğŸ“¦ research-paper-scraper
 â”£ ğŸ“‚ data
 â”ƒ â”£ ğŸ“‚ pdfs                # Downloaded research papers (PDF format)
 â”ƒ â”£ ğŸ“œ papers_metadata.csv  # Metadata of scraped papers
 â”ƒ â”£ ğŸ“œ annotations          # Category-wise CSV files
 â”ƒ â”ƒ â”£ ğŸ“œ Deep_Learning.csv
 â”ƒ â”ƒ â”£ ğŸ“œ Computer_Vision.csv
 â”ƒ â”ƒ â”£ ğŸ“œ NLP.csv
 â”ƒ â”ƒ â”£ ğŸ“œ Reinforcement_Learning.csv
 â”ƒ â”ƒ â”— ğŸ“œ Optimization.csv
 â”£ ğŸ“œ scraper.py             # Script to scrape and download papers
 â”£ ğŸ“œ annotate_papers.py      # Script to classify papers using an LLM API
 â”£ ğŸ“œ README.md              # Project documentation
```

## ğŸ“¥ Installation
1. **Clone the repository**
   ```sh
   git clone https://github.com/yourusername/research-paper-scraper.git
   cd research-paper-scraper
   ```
2. **Install dependencies**
   ```sh
   pip install -r requirements.txt
   ```

## âš™ï¸ Configuration
1. **Set up your API Key**
   - Replace `GEMINI_API_KEY` in `annotate_papers.py` with your actual Google Gemini API key.
   ```python
   GEMINI_API_KEY = "your-api-key-here"
   ```

2. **Update file paths**
   - Ensure `BASE_PDF_PATH` and `OUTPUT_CSV` in `annotate_papers.py` point to the correct directories.

## ğŸƒ Usage
### 1ï¸âƒ£ Scrape Research Papers
Run the scraper script to download papers from NeurIPS:
```sh
python scraper.py
```

### 2ï¸âƒ£ Annotate Papers using LLM API
Run the annotation script to classify papers:
```sh
python annotate_papers.py
```

### 3ï¸âƒ£ View Annotated Papers
- Open `papers_metadata.csv` to see annotated papers.
- Individual category files are stored in `data/annotations/`.

## ğŸ“Š Categories Used for Annotation
- **Deep Learning**
- **Computer Vision**
- **Natural Language Processing (NLP)**
- **Reinforcement Learning**
- **Optimization**

## ğŸ”¥ Challenges Faced
- **API Connectivity Issues**: Encountered rate limits and quota errors.
- **Handling Large PDF Files**: Extracting structured text was difficult for some PDFs.
- **Library Compatibility**: Ensuring compatibility between Python libraries for scraping and annotation.

## ğŸ“– Blog Post
For an in-depth explanation, check out the [Medium Blog Post](#) (Replace `#` with actual link).

## ğŸ“œ License
This project is open-source and available under the [MIT License](LICENSE).

---
âš¡ **Happy Researching!** ğŸš€
